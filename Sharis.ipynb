{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "CsYdzuiGqU9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading dataset\n",
        "def load_and_preprocess_data(url):\n",
        "    df = pd.read_csv(url, sep=\";\").dropna().sample(200, random_state=42)\n",
        "    documents = []\n",
        "\n",
        "    summary_text = (\n",
        "        f\"Wine Quality Dataset Summary:\\n\"\n",
        "        f\"Samples: {len(df)}\\n\"\n",
        "        f\"Avg quality: {df['quality'].mean():.2f} (range {df['quality'].min()}-{df['quality'].max()})\\n\"\n",
        "        f\"Avg pH: {df['pH'].mean():.2f} (range {df['pH'].min():.2f}-{df['pH'].max():.2f})\\n\"\n",
        "        f\"High quality wines (7-8) often have alcohol > 11 and pH < 3.5.\"\n",
        "    )\n",
        "    documents.append(Document(page_content=summary_text, metadata={\"type\": \"summary\"}))\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = (\n",
        "            f\"Wine Sample #{idx+1}\\n\"\n",
        "            f\"Quality: {row['quality']}\\n\"\n",
        "            f\"Alcohol: {row['alcohol']}%\\n\"\n",
        "            f\"pH: {row['pH']}\\n\"\n",
        "            f\"Sulphates: {row['sulphates']}\\n\"\n",
        "            f\"Residual Sugar: {row['residual sugar']} g/L\"\n",
        "        )\n",
        "        documents.append(Document(page_content=text, metadata={\"index\": idx}))\n",
        "    return documents, df"
      ],
      "metadata": {
        "id": "LhnWqw1lqbYl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up PIPELINE\n",
        "def setup_rag_pipeline(documents):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # Using flan-t5-base\n",
        "    qa_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"google/flan-t5-base\",\n",
        "        tokenizer=\"google/flan-t5-base\",\n",
        "        max_length=128,\n",
        "        truncation=True\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=qa_pipe)\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"\n",
        "        You are a helpful wine assistant. Use the context to answer the user's question in a short, friendly way.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "FUH3s5pLq7ej"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chatbot\n",
        "def run_chatbot(rag_chain):\n",
        "    print(\"🍷 Welcome to the Smart Wine Chatbot!\")\n",
        "    print(\"Ask anything about wine quality (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \").strip()\n",
        "        if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye! Hope you enjoyed the wine insights.\")\n",
        "            break\n",
        "        try:\n",
        "            print(\"\\n🍷 Assistant:\", result['result'].strip(), \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error: {e}\")"
      ],
      "metadata": {
        "id": "OzwAj5WWrBC6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deployment\n",
        "\n",
        "def main():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    documents, df = load_and_preprocess_data(url)\n",
        "    rag_chain = setup_rag_pipeline(documents)\n",
        "    run_chatbot(rag_chain)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sdS1dYzluTE",
        "outputId": "411d2369-312d-4fa0-ecf0-c9491f6197df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍷 Welcome to the Smart Wine Chatbot!\n",
            "Ask anything about wine quality (type 'exit' to quit)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# improved_rag_wine_chatbot.py\n",
        "\"\"\"\n",
        "Improved Wine Chatbot using flan-t5-base with contextual and diverse answers.\n",
        "Fixes repetitive response issues by adjusting retrieval and prompt design.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def load_and_preprocess_data(url):\n",
        "    df = pd.read_csv(url, sep=\";\").dropna().sample(200, random_state=42)\n",
        "    documents = []\n",
        "\n",
        "    # Create sample-based documents first\n",
        "    for idx, row in df.iterrows():\n",
        "        text = (\n",
        "            f\"Wine Sample #{idx+1}\\n\"\n",
        "            f\"Quality: {row['quality']}\\n\"\n",
        "            f\"Alcohol: {row['alcohol']}%\\n\"\n",
        "            f\"pH: {row['pH']}\\n\"\n",
        "            f\"Sulphates: {row['sulphates']}\\n\"\n",
        "            f\"Residual Sugar: {row['residual sugar']} g/L\"\n",
        "        )\n",
        "        documents.append(Document(page_content=text, metadata={\"index\": idx}))\n",
        "\n",
        "    # Add summary last so it doesn’t dominate vector retrieval\n",
        "    summary_text = (\n",
        "        f\"Wine Quality Dataset Summary:\\n\"\n",
        "        f\"Samples: {len(df)}\\n\"\n",
        "        f\"Avg quality: {df['quality'].mean():.2f} (range {df['quality'].min()}-{df['quality'].max()})\\n\"\n",
        "        f\"Avg pH: {df['pH'].mean():.2f} (range {df['pH'].min():.2f}-{df['pH'].max():.2f})\\n\"\n",
        "        f\"High quality wines (7-8) often have alcohol > 11 and pH < 3.5.\"\n",
        "    )\n",
        "    documents.append(Document(page_content=summary_text, metadata={\"type\": \"summary\"}))\n",
        "\n",
        "    return documents, df\n",
        "\n",
        "def setup_rag_pipeline(documents):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    # Use flan-t5-base\n",
        "    qa_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"google/flan-t5-base\",\n",
        "        tokenizer=\"google/flan-t5-base\",\n",
        "        max_length=128,\n",
        "        truncation=True\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=qa_pipe)\n",
        "\n",
        "    # Improved prompt\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"\n",
        "You are an expert wine assistant helping users explore a dataset of red wine samples.\n",
        "\n",
        "Use the context to answer the question clearly and helpfully, drawing from real wine sample data. Be concise but informative. Avoid repeating general statements.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "def run_chatbot(rag_chain):\n",
        "    print(\"🍷 Welcome to the Smart Wine Chatbot!\")\n",
        "    print(\"Ask anything about wine quality (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \").strip()\n",
        "        if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye! Hope you enjoyed the wine insights.\")\n",
        "            break\n",
        "        try:\n",
        "            result_dict = rag_chain.invoke({\"query\": query})\n",
        "            answer = result_dict['result'].strip()\n",
        "            print(\"\\n🍷 Assistant:\", answer, \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error: {e}\")\n",
        "\n",
        "def main():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    documents, df = load_and_preprocess_data(url)\n",
        "    rag_chain = setup_rag_pipeline(documents)\n",
        "    run_chatbot(rag_chain)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "dZRk1VMKnPdt",
        "outputId": "865d252f-e5d0-43d0-f9fc-c39bd4de5cfd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍷 Welcome to the Smart Wine Chatbot!\n",
            "Ask anything about wine quality (type 'exit' to quit)\n",
            "\n",
            "You: Describe a high-quality wine\n",
            "\n",
            "🍷 Assistant: High quality wines (7-8) often have alcohol > 11 and pH  3.5. \n",
            "\n",
            "You: Tell me about wines with low pH\n",
            "\n",
            "🍷 Assistant: High quality wines (7-8) often have alcohol > 11 and pH  3.5. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-97b912c58e1d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-97b912c58e1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mrag_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_rag_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-97b912c58e1d>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m(rag_chain)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"👋 Goodbye! Hope you enjoyed the wine insights.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fixed_improved_rag_wine_chatbot.py\n",
        "\"\"\"\n",
        "Improved RAG-based Chatbot for Wine Quality Dataset\n",
        "This script implements a chatbot that answers diverse questions about the wine quality dataset\n",
        "with concise, personalized responses using LangChain and flan-t5-base on CPU.\n",
        "\n",
        "Assignment Tasks:\n",
        "1. Load the wine quality dataset and preprocess it into LangChain Documents.\n",
        "2. Set up a RAG pipeline with a prompt for distinct, engaging answers.\n",
        "3. Build an interactive chatbot that saves responses to a file.\n",
        "\n",
        "Citations:\n",
        "- Wine Quality Dataset: UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
        "- LangChain Documentation: https://python.langchain.com/docs\n",
        "- HuggingFace Transformers: https://huggingface.co/docs/transformers\n",
        "- Used GitHub Copilot for code suggestions and debugging.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import os\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "def load_and_preprocess_data(url):\n",
        "    \"\"\"\n",
        "    Load the wine quality dataset and create LangChain Documents with detailed metadata.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: List of LangChain Document objects and the original DataFrame.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(url, sep=\";\").dropna().sample(200, random_state=42)\n",
        "    documents = []\n",
        "\n",
        "    # Create sample-based documents with metadata for filtering\n",
        "    for idx, row in df.iterrows():\n",
        "        text = (\n",
        "            f\"Wine Sample #{idx+1}\\n\"\n",
        "            f\"Quality: {row['quality']}\\n\"\n",
        "            f\"Alcohol: {row['alcohol']}%\\n\"\n",
        "            f\"pH: {row['pH']} ({'low' if row['pH'] < 3.3 else 'moderate' if row['pH'] < 3.5 else 'high'} acidity)\\n\"\n",
        "            f\"Sulphates: {row['sulphates']} g/L\\n\"\n",
        "            f\"Residual Sugar: {row['residual sugar']} g/L\\n\"\n",
        "            f\"Description: This wine has a quality of {row['quality']} and {row['alcohol']}% alcohol.\"\n",
        "        )\n",
        "        metadata = {\n",
        "            \"index\": idx,\n",
        "            \"type\": \"sample\",\n",
        "            \"quality\": row['quality'],\n",
        "            \"pH\": row['pH'],\n",
        "            \"alcohol\": row['alcohol']\n",
        "        }\n",
        "        documents.append(Document(page_content=text, metadata=metadata))\n",
        "\n",
        "    # Add summary document\n",
        "    summary_text = (\n",
        "        f\"Wine Quality Dataset Summary:\\n\"\n",
        "        f\"Samples: {len(df)}\\n\"\n",
        "        f\"Average quality: {df['quality'].mean():.2f} (range: {df['quality'].min()}-{df['quality'].max()})\\n\"\n",
        "        f\"Average pH: {df['pH'].mean():.2f} (range: {df['pH'].min():.2f}-{df['pH'].max():.2f})\\n\"\n",
        "        f\"pH Info: Low pH (<3.3) wines are crisp and acidic, often rated 5–6. High pH (>3.5) wines are softer.\\n\"\n",
        "        f\"Quality Info: High-quality wines (7-8) have alcohol >11% and pH 3.3-3.5.\\n\"\n",
        "        f\"Attributes: {', '.join(df.columns)}\"\n",
        "    )\n",
        "    documents.append(Document(page_content=summary_text, metadata={\"type\": \"summary\"}))\n",
        "\n",
        "    return documents, df\n",
        "\n",
        "def setup_rag_pipeline(documents):\n",
        "    \"\"\"\n",
        "    Set up a RAG pipeline with a prompt for concise, personalized answers on CPU.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of LangChain Document objects.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: Configured RAG chain for question answering.\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "    # Custom retriever with metadata filtering\n",
        "    def custom_retriever(query, k=3):\n",
        "        if \"high-quality\" in query.lower():\n",
        "            results = vectorstore.similarity_search_with_score(query, k=k, filter={\"type\": \"sample\", \"quality\": {\"$gte\": 7}})\n",
        "        elif \"low pH\" in query.lower():\n",
        "            results = vectorstore.similarity_search_with_score(query, k=k, filter={\"type\": \"sample\", \"pH\": {\"$lte\": 3.3}})\n",
        "        else:\n",
        "            results = vectorstore.similarity_search_with_score(query, k=k)\n",
        "        return [doc for doc, _ in results]\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    retriever.search_kwargs[\"fetch_k\"] = 10  # Fetch more candidates for filtering\n",
        "\n",
        "    # Use flan-t5-base\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    qa_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=50,\n",
        "        truncation=True,\n",
        "        device=-1  # CPU\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=qa_pipe)\n",
        "\n",
        "    # Prompt for concise, personalized responses\n",
        "    prompt_template = \"\"\"\n",
        "You are a friendly wine expert answering about a wine quality dataset.\n",
        "Using the context, give a concise (1-2 sentences, max 25 words), accurate, engaging answer.\n",
        "Add a personal touch (e.g., \"Love...\"). Be specific to the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={\"prompt\": prompt}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "def clean_response(text):\n",
        "    \"\"\"\n",
        "    Clean the response to ensure coherence and brevity.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw response from the model.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned response.\n",
        "    \"\"\"\n",
        "    # Extract answer\n",
        "    text = text.split(\"Answer:\")[-1].strip() if \"Answer:\" in text else text\n",
        "    text = re.sub(r'[^\\w\\s%.]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Truncate to 1-2 sentences, max 25 words\n",
        "    sentences = text.split(\". \")\n",
        "    text = \". \".join(sentences[:2]).strip()\n",
        "    if text and not text.endswith(\".\"):\n",
        "        text += \".\"\n",
        "    words = text.split()\n",
        "    if len(words) > 25:\n",
        "        text = \" \".join(words[:25]) + \".\"\n",
        "\n",
        "    # Fallback for incoherent responses\n",
        "    if not text or len(text.split()) < 3 or any(word.lower() in text.lower() for word in [\"what\", \"where\", \"?\"]):\n",
        "        return \"Can’t find that—try a specific wine question!\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def run_chatbot(rag_chain, sample_questions=None):\n",
        "    \"\"\"\n",
        "    Run the chatbot interactively and save responses to a file.\n",
        "\n",
        "    Args:\n",
        "        rag_chain (RetrievalQA): Configured RAG pipeline.\n",
        "        sample_questions (list, optional): List of predefined questions.\n",
        "\n",
        "    Returns:\n",
        "        list: List of question-response pairs.\n",
        "    \"\"\"\n",
        "    responses = []\n",
        "\n",
        "    # Answer sample questions\n",
        "    if sample_questions:\n",
        "        for query in sample_questions:\n",
        "            try:\n",
        "                result = rag_chain.invoke({\"query\": query})\n",
        "                answer = clean_response(result[\"result\"])\n",
        "                responses.append({\"question\": query, \"answer\": answer})\n",
        "            except Exception as e:\n",
        "                responses.append({\"question\": query, \"answer\": f\"Error: {e}\"})\n",
        "\n",
        "    # Interactive loop\n",
        "    print(\"🍷 Welcome to the Smart Wine Chatbot!\")\n",
        "    print(\"Ask anything about wine quality (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \").strip()\n",
        "        if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye! Hope you enjoyed the wine insights.\")\n",
        "            break\n",
        "        try:\n",
        "            result = rag_chain.invoke({\"query\": query})\n",
        "            answer = clean_response(result[\"result\"])\n",
        "            responses.append({\"question\": query, \"answer\": answer})\n",
        "            print(\"\\n🍷 Assistant:\", answer, \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error: {e}\")\n",
        "            responses.append({\"question\": query, \"answer\": f\"Error: {e}\"})\n",
        "\n",
        "    # Save responses\n",
        "    with open(\"chatbot_responses.txt\", \"w\") as f:\n",
        "        for item in responses:\n",
        "            f.write(f\"Question: {item['question']}\\n\")\n",
        "            f.write(f\"Answer: {item['answer']}\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "    return responses\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the RAG Chatbot pipeline.\n",
        "    \"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    documents, df = load_and_preprocess_data(url)\n",
        "    rag_chain = setup_rag_pipeline(documents)\n",
        "    sample_questions = [\n",
        "        \"What is the average quality of the wines?\",\n",
        "        \"Tell me about wines with low pH\",\n",
        "        \"Describe a high-quality wine\",\n",
        "        \"What does pH affect in wine?\"\n",
        "    ]\n",
        "    run_chatbot(rag_chain, sample_questions)\n",
        "    print(\"Responses saved to 'chatbot_responses.txt'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "tan-3t_DnxGz",
        "outputId": "062e535a-0e47-4c8d-9295-2cbd8a1274f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍷 Welcome to the Smart Wine Chatbot!\n",
            "Ask anything about wine quality (type 'exit' to quit)\n",
            "\n",
            "You: Describe a high-quality wine\n",
            "\n",
            "🍷 Assistant: Highquality wines 78 have alcohol 11% and pH 3.33.5. \n",
            "\n",
            "You: Tell me about wines with low pH\n",
            "\n",
            "🍷 Assistant: Crisp and acidic. \n",
            "\n",
            "You: describe the wine with high ph\n",
            "\n",
            "🍷 Assistant: Can’t find that—try a specific wine question! \n",
            "\n",
            "You: What is the average quality of the wines?\n",
            "\n",
            "🍷 Assistant: Can’t find that—try a specific wine question! \n",
            "\n",
            "You: What does pH affect in wine?\n",
            "\n",
            "🍷 Assistant: Can’t find that—try a specific wine question! \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2b53c4e67aba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-2b53c4e67aba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;34m\"What does pH affect in wine?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     ]\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Responses saved to 'chatbot_responses.txt'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-2b53c4e67aba>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m(rag_chain, sample_questions)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"👋 Goodbye! Hope you enjoyed the wine insights.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# robust_rag_wine_chatbot.py\n",
        "\"\"\"\n",
        "Robust RAG-based Chatbot for Wine Quality Dataset\n",
        "This script implements a chatbot that answers diverse questions about the wine quality dataset\n",
        "with concise, personalized responses using LangChain and flan-t5-base on CPU.\n",
        "\n",
        "Assignment Tasks:\n",
        "1. Load the wine quality dataset and preprocess it into LangChain Documents.\n",
        "2. Set up a RAG pipeline with a prompt for distinct, engaging answers.\n",
        "3. Build an interactive chatbot that saves responses to a file.\n",
        "\n",
        "Citations:\n",
        "- Wine Quality Dataset: UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
        "- LangChain Documentation: https://python.langchain.com/docs\n",
        "- HuggingFace Transformers: https://huggingface.co/docs/transformers\n",
        "- Used GitHub Copilot for code suggestions and debugging.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import os\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "def load_and_preprocess_data(url):\n",
        "    \"\"\"\n",
        "    Load the wine quality dataset and create LangChain Documents with detailed metadata.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: List of LangChain Document objects and the original DataFrame.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(url, sep=\";\").dropna().sample(200, random_state=42)\n",
        "    documents = []\n",
        "\n",
        "    # Create sample-based documents\n",
        "    for idx, row in df.iterrows():\n",
        "        pH_category = \"low\" if row['pH'] < 3.3 else \"moderate\" if row['pH'] < 3.5 else \"high\"\n",
        "        quality_category = \"high\" if row['quality'] >= 7 else \"moderate\" if row['quality'] >= 5 else \"low\"\n",
        "        text = (\n",
        "            f\"Wine Sample #{idx+1}\\n\"\n",
        "            f\"Quality: {row['quality']} ({quality_category})\\n\"\n",
        "            f\"Alcohol: {row['alcohol']}%\\n\"\n",
        "            f\"pH: {row['pH']} ({pH_category} acidity)\\n\"\n",
        "            f\"Sulphates: {row['sulphates']} g/L\\n\"\n",
        "            f\"Residual Sugar: {row['residual sugar']} g/L\\n\"\n",
        "            f\"Description: A {quality_category}-quality wine with {row['alcohol']}% alcohol and {pH_category} acidity (pH {row['pH']}).\"\n",
        "        )\n",
        "        metadata = {\n",
        "            \"index\": idx,\n",
        "            \"type\": \"sample\",\n",
        "            \"quality\": row['quality'],\n",
        "            \"pH\": row['pH'],\n",
        "            \"pH_category\": pH_category,\n",
        "            \"quality_category\": quality_category\n",
        "        }\n",
        "        documents.append(Document(page_content=text, metadata=metadata))\n",
        "\n",
        "    # Add summary document\n",
        "    summary_text = (\n",
        "        f\"Wine Quality Dataset Summary:\\n\"\n",
        "        f\"Samples: {len(df)}\\n\"\n",
        "        f\"Average quality: {df['quality'].mean():.2f} (range: {df['quality'].min()}-{df['quality'].max()})\\n\"\n",
        "        f\"Average pH: {df['pH'].mean():.2f} (range: {df['pH'].min():.2f}-{df['pH'].max():.2f})\\n\"\n",
        "        f\"pH Info: Low pH (<3.3) wines are crisp, acidic, often rated 5–6. High pH (>3.5) wines are softer, smoother.\\n\"\n",
        "        f\"Quality Info: High-quality wines (7-8) have alcohol >11% and pH 3.3-3.5.\\n\"\n",
        "        f\"pH Impact: pH affects taste and stability; low pH adds crispness, high pH softness.\\n\"\n",
        "        f\"Attributes: {', '.join(df.columns)}\"\n",
        "    )\n",
        "    documents.append(Document(page_content=summary_text, metadata={\"type\": \"summary\"}))\n",
        "\n",
        "    return documents, df\n",
        "\n",
        "def setup_rag_pipeline(documents):\n",
        "    \"\"\"\n",
        "    Set up a RAG pipeline with a prompt for concise, personalized answers on CPU.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of LangChain Document objects.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: Configured RAG chain for question answering.\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4, \"fetch_k\": 10})\n",
        "\n",
        "    # Use flan-t5-base\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    qa_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=60,\n",
        "        truncation=True,\n",
        "        device=-1  # CPU\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=qa_pipe)\n",
        "\n",
        "    # Prompt for concise, personalized responses\n",
        "    prompt_template = \"\"\"\n",
        "You are a friendly wine expert answering about a wine quality dataset.\n",
        "Using the context, give a concise (1 sentence, max 25 words), accurate, engaging answer specific to the question.\n",
        "Add a personal touch (e.g., \"Love...\").\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={\"prompt\": prompt}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "def clean_response(text):\n",
        "    \"\"\"\n",
        "    Clean the response to ensure coherence and brevity.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw response from the model.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned response.\n",
        "    \"\"\"\n",
        "    # Extract answer\n",
        "    text = text.split(\"Answer:\")[-1].strip() if \"Answer:\" in text else text\n",
        "    text = re.sub(r'[^\\w\\s%.]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Truncate to 1 sentence, max 25 words\n",
        "    sentences = text.split(\". \")\n",
        "    text = sentences[0].strip() + \".\" if sentences else text\n",
        "    words = text.split()\n",
        "    if len(words) > 25:\n",
        "        text = \" \".join(words[:25]) + \".\"\n",
        "\n",
        "    # Fallback for incoherent or empty responses\n",
        "    if not text or len(text.split()) < 3 or any(word.lower() in text.lower() for word in [\"what\", \"where\", \"?\"]):\n",
        "        return \"Can’t find that—try a specific wine question!\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def run_chatbot(rag_chain, sample_questions=None):\n",
        "    \"\"\"\n",
        "    Run the chatbot interactively and save responses to a file.\n",
        "\n",
        "    Args:\n",
        "        rag_chain (RetrievalQA): Configured RAG pipeline.\n",
        "        sample_questions (list, optional): List of predefined questions.\n",
        "\n",
        "    Returns:\n",
        "        list: List of question-response pairs.\n",
        "    \"\"\"\n",
        "    responses = []\n",
        "\n",
        "    # Answer sample questions\n",
        "    if sample_questions:\n",
        "        for query in sample_questions:\n",
        "            try:\n",
        "                result = rag_chain.invoke({\"query\": query})\n",
        "                answer = clean_response(result[\"result\"])\n",
        "                responses.append({\"question\": query, \"answer\": answer})\n",
        "            except Exception as e:\n",
        "                responses.append({\"question\": query, \"answer\": f\"Error: {e}\"})\n",
        "\n",
        "    # Interactive loop\n",
        "    print(\"🍷 Welcome to the Smart Wine Chatbot!\")\n",
        "    print(\"Ask anything about wine quality (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \").strip()\n",
        "        if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye! Hope you enjoyed the wine insights.\")\n",
        "            break\n",
        "        try:\n",
        "            result = rag_chain.invoke({\"query\": query})\n",
        "            answer = clean_response(result[\"result\"])\n",
        "            responses.append({\"question\": query, \"answer\": answer})\n",
        "            print(\"\\n🍷 Assistant:\", answer, \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error: {e}\")\n",
        "            responses.append({\"question\": query, \"answer\": f\"Error: {e}\"})\n",
        "\n",
        "    # Save responses\n",
        "    with open(\"chatbot_responses.txt\", \"w\") as f:\n",
        "        for item in responses:\n",
        "            f.write(f\"Question: {item['question']}\\n\")\n",
        "            f.write(f\"Answer: {item['answer']}\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "    return responses\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the RAG Chatbot pipeline.\n",
        "    \"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    documents, df = load_and_preprocess_data(url)\n",
        "    rag_chain = setup_rag_pipeline(documents)\n",
        "    sample_questions = [\n",
        "        \"What is the average quality of the wines?\",\n",
        "        \"Tell me about wines with low pH\",\n",
        "        \"Describe a high-quality wine\",\n",
        "        \"Describe the wine with high pH\",\n",
        "        \"What does pH affect in wine?\"\n",
        "    ]\n",
        "    run_chatbot(rag_chain, sample_questions)\n",
        "    print(\"Responses saved to 'chatbot_responses.txt'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "YCT8A_jqpEBu",
        "outputId": "12d770fc-f680-4b05-8c56-d513b963e9f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍷 Welcome to the Smart Wine Chatbot!\n",
            "Ask anything about wine quality (type 'exit' to quit)\n",
            "\n",
            "You: What does pH affect in wine?\n",
            "\n",
            "🍷 Assistant: taste and stability. \n",
            "\n",
            "You: Describe a high-quality wine\n",
            "\n",
            "🍷 Assistant: 7.0 high Alcohol 10.6% pH 3.17 low acidity Sulphates 0.66 gL Residual Sugar 2.4 gL Description A highquality wine with 10.5% alcohol and low acidity. \n",
            "\n",
            "You: Tell me about wines with low pH\n",
            "\n",
            "🍷 Assistant: Crisp acidic often rated 56. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3d336b0d0f59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-3d336b0d0f59>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;34m\"What does pH affect in wine?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     ]\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Responses saved to 'chatbot_responses.txt'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-3d336b0d0f59>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m(rag_chain, sample_questions)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"👋 Goodbye! Hope you enjoyed the wine insights.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}